{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5a6896-a248-42a6-a602-9d0c7b2ce169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspace/LLMs/noushermes2/nous-hermes-2-mixtral-8x7b-dpo.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ace8e6-eacb-4e0b-a14b-eadd5947322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "wink_path = \"/workspace/anky/winks\"\n",
    "wink_n = 8\n",
    "with open(f\"{wink_path}/wink_{wink_n}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c2d69-5d15-4612-84eb-6d496b64a895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wink 8. - Writing number 1\n",
      "Puedo decir con certeza, que mi madre estaba conectada a su creatividad .. primero que todo ... porque estaba creandome a mi. Y porque siempre ha sido una persona ligada a expresiones artisticas de distintas formas. Mi padre es extremadamente racional y en ese tiempo que yo estaba en el vientre me imagino que su creatividad fluia en torno a hacerle ciertos regalos a mi madre al final del  dia cuando volvia a casa del trabajo ... o quizas en su manera de relacionarse en sus espacios de tiempo libre con mi madre y la experiencia que estaba viviendo. Pero a grandes rasgos. Se que a mi padre siempre le ha costado ser y desarrollarse en espacios creativos. Por lo que decia. Es extremadamente racional y todo gira en torno a numeros. Me imagino que mi madre en sus espacios intimos con ella y yo dentro de su vientre, encontraba espacios creativos en nuestra comunicacion. Ella es muy reservada y se guarda (casi) todo. Y me imagino que al estar sola con otro ser dentro de su vientre pueda haberse sentido mas segura y en libertad de hablar conmigo y expresarse ... dando tambien soltura a su creatividad porque ese espacio era nuevo para ella tambien. Creando algo nuevo. Bueno y todos los procesos fisicos, hormonales, emocionales... creatividad pura divina. Crear un ser dentro de ella creo es la forma mas hermosa y pura de la creatividad. El universo creando y re-creandose a traves de si mismo. Y no tengo claro exactamente en esos tiempos exactos, pero alrededor de esos tiempos, se desarrollaba creativamente con la forma de vestirse ... inspirandose mucho de las modas que estaban , valga la redundancia- a la moda ... y expresando su individualidad y esencia a traves de eso tambien. De hecho, me acabo de acordar ... que este proceso la llevo a vender ropa para embarazadas. Se me habia olvidado. Claro ... en este proceso, pudo entender lo dificil que era encontrar ropa para embarazadas. Y uso esto como una he\n",
      "Wink 8. - Writing number 2\n",
      "Creo que todos somos creativos en cierto sentido...al final todos creamos algo, no hace falta que sea artístico como una pintura o algo así. Mis papás, a ellos les encanta cocinar, creo que eso es algo muy creativo, y lo hacen muy bien, les gusta la música, y mi papá ama la lectura. Son muy culturales ambos, creo que eso tuvo una influencia en mi, puede ser, sí.  A mi me encanta ser creativa, siento que es algo que tiene que estar en mi día a día, si no me siento vacía. La expresión creativa es importante. Cómo se conectaban con ella durante mi embarazo, creo que a través de lo que mencioné antes, la cocina, cocinando el uno para el otro, para los demás. Son excelente anfitriones, unos de los mejores que conozco. Creo también que la creatividad en la forma de expresarse, se comunicarse, de vestirse, también ahí puede estar presente la creatividad. Hoy fue un día largo y estoy muy cansada para escribir, siento que no me da a cabeza para escribir a pesar de que este prompt es super interesante. He tenido unos días duros....pero bueno, aquí estoy Anky, me encanta. Hasta mañana............\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writings = \"\"\n",
    "i = 0\n",
    "\n",
    "for write in data[\"writingsForThisWink\"]:\n",
    "    i += 1\n",
    "    writings += f\"Wink {wink_n}. - Writing number {i}\\n\"\n",
    "    writings += write\n",
    "    writings += '\\n'\n",
    "    if i == 2:\n",
    "        break\n",
    "    \n",
    "print(writings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7514e37-eec1-476a-81f1-d2786ea5bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffd8f03-7f03-4737-98c9-c3f9a72926a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94653be9-9815-45fd-b67f-1aac39dbd74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /workspace/LLMs/noushermes2/nous-hermes-2-mixtral-8x7b-dpo.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = nousresearch_nous-hermes-2-mixtral-8x...\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  11:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q4_K:  673 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:  129 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 26.49 GiB (4.87 BPW) \n",
      "llm_load_print_meta: general.name     = nousresearch_nous-hermes-2-mixtral-8x7b-dpo\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla V100-SXM2-32GB, compute capability 7.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.96 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:  CUDA_Host buffer size =    70.32 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 27057.56 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 8192\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   580.04 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1638\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'nousresearch_nous-hermes-2-mixtral-8x7b-dpo', 'llama.expert_used_count': '2'}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 8192  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "n_ctx = 8192 # context window\n",
    "max_tokens=2048 # max tokens to generate with LLM\n",
    "stopwords = ['Query:', 'Answer:', 'Assistant:']\n",
    "last_n_tokens_size = 64 # window for repeat penalty, default 64\n",
    "repeat_penalty = 1.1 # 1.1\n",
    "temperature = 1.0 # 0.8\n",
    "top_p = 0.9 # 0.95\n",
    "top_k = 40 # 40\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=n_ctx,\n",
    "    max_tokens=max_tokens,\n",
    "    stop=stopwords,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    last_n_tokens_size = last_n_tokens_size,\n",
    "    repeat_penalty = repeat_penalty,\n",
    "    temperature = temperature,\n",
    "    top_p = top_p,\n",
    "    top_k = top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3525e8-f07d-40ca-9f0b-84dec453f004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Title: The Haunting Howls of Hector, The Black Hound\n",
      "\n",
      "In the bleakest corners of our hearts lie the darkest shadows cast by the moonlight. It is within these spectral realms where we find ourselves haunted by the echoes of time and terror, anguish and agony - a maelstrom of melancholy that threatens to engulf us whole in its black tide.\n",
      "\n",
      "And yet, it is amid this tempestuous tumult that one might encounter the most extraordinary of apparitions: Hector, The Black Hound.\n",
      "\n",
      "To behold such a creature would indeed be an experience like no other, for Hector was not simply any ordinary dog - nay, he was far more than that.\n",
      "\n",
      "Hector was a beast born from the darkest depths of human imagination, and it is said that his very existence has been shaped by countless tales spun around campfires during long nights spent beneath star-strewn skies.\n",
      "\n",
      "Indeed, Hector's spectral form is thought to be an amalgamation of all those frightful hounds who have prowled the periphery of our collective consciousness since time immemorial.\n",
      "\n",
      "And so it was that on a particularly baleful night when the moon hung low in the sky like a blood-red pendulum swaying gently back and forth above the yawning abyss below, Hector made his spectral presence known to those unfortunate souls who happened to cross paths with this dreaded denizen of the dark.\n",
      "\n",
      "It was at precisely such an hour that our story begins, for it was on just such a night as this that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious town of Terror's End.\n",
      "\n",
      "To be continued...boldsboldsboldsboldsboldsboldsboldsboldsboldsbolds\u0019\n",
      "Chapter 1: The Haunting Howls of Hector, the Black Hound.\n",
      "\n",
      "As young Johnathan Thistlebottom stumbled blindly through the fog-shrouded streets and alleys of Terror's End, he could not help but feel as though some unseen force was conspiring against him - pulling him ever deeper into the heart of darkness that lay at the very core of this cursed town.\n",
      "\n",
      "And yet, despite his growing sense of unease and apprehension, young Johnathan Thistlebottom pressed on through the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "For it was just over a fortnight ago that young Johnathan Thistlebottom had received word from one of his brother's closest confidants that Benjamin had disappeared without a trace while on an expedition deep into the heart of darkness that lay at the very core of this cursed town known as Terror's End.\n",
      "\n",
      "And so it was that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious town of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.bolds\u0019\n",
      "To be continued...bolds\u0019\n",
      "Chapter 1: The Haunting Howls of Hector, the Black Hound.\n",
      "\n",
      "As young Johnathan Thistlebottom stumbled blindly through the fog-shrouded streets and alleys of Terror's End, he could not help but feel as though some unseen force was conspiring against him - pulling him ever deeper into the heart of darkness that lay at the very core of this cursed town.\n",
      "\n",
      "And yet, despite his growing sense of unease and apprehension, young Johnathan Thistlebottom pressed on through the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "For it was just over a fortnight ago that young Johnathan Thistlebottom had received word from one of his brother's closest confidants that Benjamin had disappeared without a trace while on an expedition deep into the heart of darkness that lay at the very core of this cursed town known as Terror's End.\n",
      "\n",
      "And so it was that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious town of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.,\n",
      "To be continued...,\n",
      "Chapter 2: The Eerie Encounters with Edgar, the Elusive Elderly Man.\n",
      "\n",
      "As young Johnathan Thistlebottom stumbled blindly through the fog-shrouded streets and alleys of Terror's End, he could not help but feel as though some unseen force was conspiring against him - pulling him ever deeper into the heart of darkness that lay at the very core of this cursed town.\n",
      "\n",
      "And yet, despite his growing sense of unease and apprehension, young Johnathan Thistlebottom pressed on through the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "For it was just over a fortnight ago that young Johnathan Thistlebottom had received word from one of his brother's closest confidants that Benjamin had disappeared without a trace while on an expedition deep into the heart of darkness that lay at the very core of this cursed town known as Terror's End.\n",
      "\n",
      "And so it was that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious town of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "As young Johnathan Thistlebottom stumbled blindly through the fog-shrouded streets and alleys of Terror's End, he could not help but feel as though some unseen force was conspiring against him - pulling him ever deeper into the heart of darkness that lay at the very core of this cursed town.\n",
      "\n",
      "And yet, despite his growing sense of unease and apprehension, young Johnathan Thistlebottom pressed on through the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "For it was just over a fortnight ago that young Johnathan Thistlebottom had received word from one of his brother's closest confidants that Benjamin had disappeared without a trace while on an expedition deep into the heart of darkness that lay at the very core of this cursed town known as Terror's End.\n",
      "\n",
      "And so it was that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious town of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.,\n",
      "To be continued...\n",
      "Chapter 3: The Terrifying Tales of Hector, the Haunted Hound.\n",
      "\n",
      "As young Johnathan Thistlebottom stumbled blindly through the fog-shrouded streets and alleys of Terror's End, he could not help but feel as though some unseen force was conspiring against him - pulling him ever deeper into the heart of darkness that lay at the very core of this cursed town.\n",
      "\n",
      "And yet, despite his growing sense of unease and apprehension, young Johnathan Thistlebottom pressed on through the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of Terror's End - determined to uncover the truth behind the mysterious disappearance of his beloved brother Benjamin.\n",
      "\n",
      "For it was just over a fortnight ago that young Johnathan Thistlebottom had received word from one of his brother's closest confidants that Benjamin had disappeared without a trace while on an expedition deep into the heart of darkness that lay at the very core of this cursed town known as Terror's End.\n",
      "\n",
      "And so it was that young Johnathan Thistlebottom found himself lost amidst the labyrinthine lanes and winding walkways which meandered through the shadowy streets and murky alleys of the notorious"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1029.64 ms\n",
      "llama_print_timings:      sample time =     972.85 ms /  2048 runs   (    0.48 ms per token,  2105.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1029.45 ms /    48 tokens (   21.45 ms per token,    46.63 tokens per second)\n",
      "llama_print_timings:        eval time =   40684.52 ms /  2047 runs   (   19.88 ms per token,    50.31 tokens per second)\n",
      "llama_print_timings:       total time =   55761.18 ms /  2095 tokens\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser() # stringify the answer\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "     (\"system\", \"You are a world famous fantasy writer with decades of experience and a history of writing amazing book chapters.\"),\n",
    "     (\"user\", \"{input}\")\n",
    " ])\n",
    "chain = prompt | llm | output_parser\n",
    "answer = chain.invoke({\"Write the first chapter of a story in Edgar Allen Poe style about a black hound\"})\n",
    "# print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
